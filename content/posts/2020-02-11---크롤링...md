---
title: 크롤링..

date: "2020-02-11T22:15:30.169Z"
template: "post"
draft: false
slug: "2020_02_11"
category: "Daily"
tags:
  - "git_hub"
  - "gatsby"
  - "Web Development"
description: "깃허브 관련 명령어 정리"
socialImage: ""
---
## 크롤링.. 삽질의 기록.... 😠😡🤬🤯
크롤링에 앞서 가상환경을 만들고 활성화해준다.
miniconda를 기준으로, 새로운 가상환경은 아래 명령어로 만들 수 있다.
```conda create -n 가상환경 이름 python=3.8```
❗️ 파이썬 버전을 명시하지 않으면 자동으로 2.x 버전이 설치되므로 주의

가상환경이 활성화 된 상태에서 bs4 request를 설치해준다.
```pip install bs4, request```

✔️ 동기님이 알려주신 가상환경 활성화 + 프로젝트 디렉토리 이동 꿀팁
![](https://images.velog.io/images/k904808/post/cebfbf2f-21d2-41b1-873a-23d8412816f9/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202020-02-11%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%209.18.57.png)

터미널에서 "scrap"이라는 명령어를 입력하면, scrap이라는 가상환경을 활성화 한 후 scrap 디렉토리로 이동하도록 설정해두었다. '.zshrc' 에서 alias 이용해 원하는 명령어와 원하는 동작을 미리 설정해두는 방법. 진짜 편함.🤩

다음으로, 프로젝트 디렉토리에 새로운 파일을 생성한다.
새로운 파일을 생성하는 방법은 크게 두 가지가 있다.

```vi 새로운파일.py``` :
명령어를 입력하면 새로운파일 이름으로 에디터가 열린다.
그 화면에서 바로 편집하고 저장하면 된다.

```touch 새로운파일.py```:
새로운 파일이 생성된다.
에디터로 열어서 편집 후 저장하면 된다.

새로운 파일에 request와 bs4를 import 해준다.
그리고 리퀘스트를 잘 찍어서 숩에 담아서 잘 가져온다. ㅋㅋㅋㅋ 말은 쉽다..ㅋㅋㅋㅋ

```
import requests
from bs4 import BeautifulSoup
```

```
req = requests.get('https:// #크롤링할 url)

html = req.text                            #request를 텍스트 형식으로 바꿔줌

soup = BeautifulSoup(html, 'html.parser')  #텍스트를 사람이 보기좋은 상태로 가공해 줌.
```

각 과정에서 데이터가 잘 따라오는지 알아보려면 print 해보는게 제일..

![](https://images.velog.io/images/k904808/post/5a1dc8d3-3a65-48e7-b227-76b2212a7c6f/image.png)

```
변수1 = soup.select(
	#개발자 도구를 통해 긁어온 css selector
)
변수2 = soup.select(
	#개발자 도구를 통해 긁어온 css selector
)
변수3 = soup.select(
	#개발자 도구를 통해 긁어온 css selector
)

빈_chart = []
for item in zip(변수1, 변수2, 변수3):
    빈_chart.append(
        {
            '변수1'  : item[0].text,
            '변수2'  : item[1].text,
            '변수3': item[2].text,
        }
    )
    for i in 빈_chart:     #이제는 안 비었지만..
        print(i)
```

이렇게 하면 크롤링 완료.
끝.
